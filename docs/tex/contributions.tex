\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{a4paper, margin=1in}

\title{Research Contributions: Physics-Informed Graph Mamba}
\author{Physics-Informed Graph Mamba Team}
\date{\today}

\begin{document}

\maketitle

\section*{Executive Summary}
This work introduces \textbf{Physics-Informed Graph Mamba (PI-GraphMamba)}, a scalable data-driven framework for joint parameter and state estimation in power distribution grids. By replacing the quadratic complexity of Transformers with the linear-time \textit{Selective State Space Model} (Mamba) and integrating it with Graph Neural Networks (GNNs), we achieve a paradigm shift from iterative, computationally expensive filtering to instantaneous, robust estimation.

\section*{Core Technical Contributions}

\subsection*{1. Spatio-Temporal Graph Mamba Architecture}
We propose a novel hybrid architecture that fuses spatial topology learning with efficient temporal sequence modeling:
\begin{itemize}
    \item \textbf{Spatial Encoding (GNN):} At each time step $t$, a Graph Convolutional Network (GCN) encodes the grid state $x_t$ and topology $\mathcal{A}$:
    \begin{equation}
        h_t = \text{GCN}(x_t, \mathcal{A})
    \end{equation}
    This allows the model to explicitly handle non-Euclidean grid structures and adapt to topology changes (e.g., line trips) without retraining.

    \item \textbf{Temporal Evolution (Mamba SSM):} We employ the Mamba block to process the sequence of embeddings $h_{1:T}$. Unlike RNNs or Transformers, Mamba uses a selective discretization of the continuous state space model:
    \begin{align}
        h'_t &= \mathbf{A}h_t + \mathbf{B}x_t \\
        y_t &= \mathbf{C}h_t
    \end{align}
    Crucially, the discretization parameters $(\Delta, \mathbf{B}, \mathbf{C})$ are input-dependent, allowing the model to selectively \textit{remember} persistent parameter drifts while \textit{ignoring} transient measurement noise.
\end{itemize}

\subsection*{2. Physics-Informed Regularization}
To ensure physical consistency in the "black-box" predictions, we implement a hybrid loss function:
\begin{equation}
    \mathcal{L} = \underbrace{||\hat{p} - p_{true}||^2}_{\text{Data Loss}} + \lambda \cdot \underbrace{||\mathbf{z} - f_{PF}(\hat{x}, \hat{p})||^2}_{\text{Physics Loss}}
\end{equation}
where $f_{PF}$ represents the AC power flow equations. This regularizes the search space, guiding the model toward solutions that satisfy Kirchhoff's laws.

\subsection*{3. Practical Implementation & Accessibility}
Our codebase prioritizes usability and reproducibility:
\begin{itemize}
    \item \textbf{Hardware-Aware Fallback:} The implementation automatically detects available hardware. While Mamba is prioritized for CUDA-enabled GPUs, the system gracefully falls back to an LSTM-based architecture on CPU-only environments, ensuring accessibility for prototyping.
    \item \textbf{Experiment Tracking:} Integrated support for \texttt{SwanLab} enables real-time monitoring of training metrics, loss curves, and validation benchmarks.
\end{itemize}

\section*{Performance Highlights}

\subsection*{1. Superior Accuracy and "One-Shot" Calibration}
Experimental results on the IEEE 33-bus system demonstrate significant improvements over the baseline Improved Adaptive Unscented Kalman Filter (IAUKF):

\begin{table}[h]
\centering
\caption{Performance Comparison (IEEE 33-Bus)}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Parameter} & \textbf{MAE} & \textbf{Improvement} \\
\midrule
IAUKF (Baseline) & R & 0.0152 & - \\
\textbf{PI-GraphMamba} & \textbf{R} & \textbf{0.0045} & \textbf{70.4\%} \\
\bottomrule
\end{tabular}
\end{table}

While IAUKF requires $\approx 80$ steps to converge, PI-GraphMamba provides accurate estimates almost instantaneously ("one-shot calibration").

\subsection*{2. Robustness to Non-Ideal Conditions}
\begin{itemize}
    \item \textbf{Non-Gaussian Noise:} The selective scan mechanism effectively gates heavy-tailed noise (outliers), preventing the oscillations observed in Kalman Filters.
    \item \textbf{Topology Shifts:} The GNN encoder explicitly accounts for the adjacency matrix, allowing the model to maintain accuracy even after line trip events, where fixed-model filters often diverge.
\end{itemize}

\end{document}
